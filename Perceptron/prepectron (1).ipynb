{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Introduction:\n\nImagine a basketball enthusiast who wants to learn how to determine whether a player is good at three-point shooting. However, this individual has no clue about how to evaluate a player's skills. The Perceptron algorithm comes into play as their coach, teaching them a simple yet effective way to do so.\n\nDevelopment:\n\nThe Perceptron serves as an automatic \"judge\" to assist in deciding whether a player excels at three-point shooting or not. Think of it as a set of scales with several trays. Each tray is labeled with a characteristic, such as past shooting accuracy, player's height, and their training history. In this context, the trays carry weights that represent the significance of each feature.\n\nWhen observing a player, they place their characteristics on the scales and multiply each characteristic's value by its corresponding weight. The results are then summed. If the sum surpasses a special number, known as the threshold, it is concluded that the player is skilled in three-point shooting. If the sum falls below this threshold, it is determined that they are not.\n\nThe intriguing aspect of the Perceptron is that the weights and the threshold can be adjusted to make accurate predictions. The learning process occurs by observing players for whom we know whether they are good or bad at three-point shooting, and then adjusting the weights to enhance the decision-making process.\n\nConclusion:\n\nThe Perceptron represents a basic yet valuable approach to making decisions based on player characteristics. It is effective for distinguishing between good and poor three-point shooters based on statistics. However, it does have limitations. For more complex player assessments, such as leadership skills or on-court endurance, the Perceptron might not be sufficient.\n\nBut there's no need to worry. The Perceptron is just the starting point in the exciting world of machine learning and artificial intelligence applied to basketball. Advanced models and deep neural networks can help us understand much more about the game, from predicting match outcomes to optimizing team strategies. The Perceptron serves as the initial step in an exciting journey towards a more intelligent and accurate understanding of basketball.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Initialize the Perceptron\nInitialize weights and bias randomly or with small values\n\n# Define the basketball position classes (forward, point guard, center, shooting guard, power forward)\nClasses = [\"Forward\", \"Point Guard\", \"Center\", \"Shooting Guard\", \"Power Forward\"]\n\n# Load the training dataset\nLoad training data with features (e.g., height, weight, scoring average, etc.) and labels (position)\n\n# Define the activation function (e.g., step function)\nDefine activation function (e.g., step function)\n\n# Define the learning rate\nLearning_rate = 0.1\n\n# Train the Perceptron\nFor each example in the training data:\n    Calculate the weighted sum of inputs and bias\n    Apply the activation function to the weighted sum\n    Compare the predicted class to the actual class label\n    If the prediction is incorrect:\n        Update the weights and bias based on the error and learning rate\n\n# Repeat the training process until a stopping criterion is met (e.g., a certain number of epochs or convergence)\n\n# The Perceptron is trained and can predict the basketball position of new players\n\n# To make a prediction for a new player:\nFor a new player with features (e.g., height, weight, scoring average, etc.):\n    Calculate the weighted sum of inputs and bias\n    Apply the activation function to the weighted sum\n    The predicted class is the position with the highest activation value\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import random\n\n# List of example male names\nnames = [\"John\", \"Michael\", \"James\", \"David\", \"Robert\", \"Daniel\", \"William\", \"Matthew\", \"Andrew\", \"Christopher\"]\n\n# Generate a dataset with 200 rows and 20 columns\ndataset = []\nfor _ in range(200):\n    row = [\n        random.choice(names),                 # Name (Male)\n        random.randint(20, 35),               # Age\n        random.uniform(70, 100),              # Weight (kg)\n        random.choice([\"Slim\", \"Medium\", \"Heavy\"]),  # Body Type\n        random.randint(0, 15),               # Years of Experience\n        random.uniform(20, 50),              # Three-Point Shooting Percentage\n        random.uniform(40, 80),              # Free Throw Shooting Percentage\n        random.uniform(30, 70),              # Long-Range Shooting Percentage\n        random.uniform(40, 80),              # Short-Range Shooting Percentage\n        random.randint(0, 5),                # Steals per game\n        random.randint(0, 3),                # Blocks per game\n        random.uniform(0, 5),                # Assists per game\n        random.randint(10, 40),              # Max Vertical Jump (inches)\n        random.uniform(40, 60),              # Winning Percentage with Player On Court\n        random.uniform(20, 40),              # Losing Percentage with Player On Court\n        random.randint(1, 5),                # Number of Teams in Last 3 Years\n        random.uniform(15, 25),              # Max Speed (mph)\n        random.randint(8, 15),               # Shoe Size\n        random.randint(100, 1000),           # Total Points Scored\n        random.choice([\"Forward\", \"Point Guard\", \"Center\", \"Shooting Guard\", \"Power Forward\"])  # Position\n    ]\n    dataset.append(row)\n\n# Display the first few rows of the dataset\nfor row in dataset[:5]:\n    print(row)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": "['Daniel', 30, 71.46242068244099, 'Heavy', 1, 37.76792945212584, 64.0106325783269, 40.990075201129315, 66.27438311143959, 1, 1, 1.508443953077515, 35, 50.99754374581799, 23.977403801956992, 1, 16.211219240429017, 13, 895, 'Shooting Guard']\n['Christopher', 31, 98.76580649222507, 'Heavy', 5, 34.62293828913636, 40.98263943727631, 47.204431958884406, 78.42552445693293, 4, 0, 2.2888689262851307, 37, 58.863029444769154, 30.023881818408412, 2, 24.730262834642005, 12, 152, 'Power Forward']\n['Matthew', 32, 76.59065582621277, 'Medium', 4, 33.54424797620067, 59.61538840886243, 47.5487331077106, 76.111404327545, 4, 3, 4.909353000783078, 40, 49.04566699581137, 34.54731405448423, 5, 16.546886005921714, 9, 128, 'Shooting Guard']\n['James', 34, 71.58526350594683, 'Slim', 2, 31.29953096857015, 55.975790939359314, 57.03496009322407, 57.92929068652134, 0, 3, 4.380491732455348, 12, 55.444082838781654, 31.284365760844477, 4, 22.954919396748572, 11, 276, 'Center']\n['William', 22, 76.92811076069282, 'Medium', 5, 44.358330997408515, 75.70468119753907, 32.93528283516768, 66.6835532056256, 3, 0, 1.0375185288344713, 39, 55.77853863734625, 32.302635786278124, 2, 17.986128753553412, 12, 155, 'Center']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import random\n\n\n\n# List of example male names\nnames = [\"John\", \"Michael\", \"James\", \"David\", \"Robert\", \"Daniel\", \"William\", \"Matthew\", \"Andrew\", \"Christopher\"]\n\n# Generate a dataset with 200 rows and 20 columns\ndataset = []\nfor _ in range(200):\n    row = [\n        random.choice(names),                 # Name (Male)\n        random.randint(20, 35),               # Age\n        random.uniform(70, 100),              # Weight (kg)\n        random.choice([\"Slim\", \"Medium\", \"Heavy\"]),  # Body Type\n        random.randint(0, 15),               # Years of Experience\n        random.uniform(20, 50),              # Three-Point Shooting Percentage\n        random.uniform(40, 80),              # Free Throw Shooting Percentage\n        random.uniform(30, 70),              # Long-Range Shooting Percentage\n        random.uniform(40, 80),              # Short-Range Shooting Percentage\n        random.randint(0, 5),                # Steals per game\n        random.randint(0, 3),                # Blocks per game\n        random.uniform(0, 5),                # Assists per game\n        random.randint(10, 40),              # Max Vertical Jump (inches)\n        random.uniform(40, 60),              # Winning Percentage with Player On Court\n        random.uniform(20, 40),              # Losing Percentage with Player On Court\n        random.randint(1, 5),                # Number of Teams in Last 3 Years\n        random.uniform(15, 25),              # Max Speed (mph)\n        random.randint(8, 15),               # Shoe Size\n        random.randint(100, 1000),           # Total Points Scored\n        random.choice([\"Forward\", \"Point Guard\", \"Center\", \"Shooting Guard\", \"Power Forward\"])  # Position\n    ]\n    dataset.append(row)\n\n# Dividir el dataset en entrenamiento y prueba (80% - 20%)\nrandom.shuffle(dataset)  # Mezclamos los datos para que no haya sesgo en la división\ntrain_size = int(0.8 * len(dataset))\ntrain_data = dataset[:train_size]\ntest_data = dataset[train_size:]\n\n# Definir la posición de baloncesto como clases numéricas (0-4)\nposition_mapping = {\n    \"Forward\": 0,\n    \"Point Guard\": 1,\n    \"Center\": 2,\n    \"Shooting Guard\": 3,\n    \"Power Forward\": 4\n}\n\n# Preparar los datos de entrenamiento y prueba\ntrain_features = [row[1:-1] for row in train_data]  # Excluimos el nombre y la posición\ntrain_labels = [position_mapping[row[-1]] for row in train_data]\ntest_features = [row[1:-1] for row in test_data]  # Excluimos el nombre y la posición\ntest_labels = [position_mapping[row[-1]] for row in test_data]\n\n# Implementar el Perceptrón\nclass Perceptron:\n    def __init__(self, num_inputs):\n        self.weights = [0] * num_inputs\n        self.bias = 0\n\n    def predict(self, inputs):\n        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias\n        return 1 if weighted_sum >= 0 else 0\n\n    def train(self, features, labels, learning_rate=0.1, epochs=100):\n        for _ in range(epochs):\n            for i in range(len(features)):\n                prediction = self.predict(features[i])\n                error = labels[i] - prediction\n                for j in range(len(self.weights)):\n                    self.weights[j] += learning_rate * error * features[i][j]\n                self.bias += learning_rate * error\n\n# Crear y entrenar el Perceptrón\nperceptron = Perceptron(len(train_features[0]))\nperceptron.train(train_features, train_labels, learning_rate=0.1, epochs=100)\n\n# Evaluar el Perceptrón en el conjunto de prueba\ncorrect_predictions = 0\ntotal_predictions = len(test_features)\n\nfor i in range(total_predictions):\n    prediction = perceptron.predict(test_features[i])\n    if prediction == test_labels[i]:\n        correct_predictions += 1\n\naccuracy = correct_predictions / total_predictions\nprint(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "ename": "<class 'TypeError'>",
          "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Crear y entrenar el Perceptrón\u001b[39;00m\n\u001b[1;32m     76\u001b[0m perceptron \u001b[38;5;241m=\u001b[39m Perceptron(\u001b[38;5;28mlen\u001b[39m(train_features[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 77\u001b[0m \u001b[43mperceptron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Evaluar el Perceptrón en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m     80\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mPerceptron.train\u001b[0;34m(self, features, labels, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features)):\n\u001b[0;32m---> 69\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m         error \u001b[38;5;241m=\u001b[39m labels[i] \u001b[38;5;241m-\u001b[39m prediction\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n",
            "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mPerceptron.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 63\u001b[0m     weighted_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weighted_sum \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import random\n\nnames = [\"John\", \"Michael\", \"James\", \"David\", \"Robert\", \"Daniel\", \"William\", \"Matthew\", \"Andrew\", \"Christopher\"]\n\ndataset = []\nfor _ in range(200):\n    row = [\n        random.choice(names),\n        random.randint(20, 35),\n        random.uniform(70, 100),\n        random.choice([\"Slim\", \"Medium\", \"Heavy\"]),\n        random.randint(0, 15),\n        random.uniform(20, 50),\n        random.uniform(40, 80),\n        random.uniform(30, 70),\n        random.uniform(40, 80),\n        random.randint(0, 5),\n        random.randint(0, 3),\n        random.uniform(0, 5),\n        random.randint(10, 40),\n        random.uniform(40, 60),\n        random.uniform(20, 40),\n        random.randint(1, 5),\n        random.uniform(15, 25),\n        random.randint(8, 15),\n        random.randint(100, 1000),\n        random.choice([\"Forward\", \"Point Guard\", \"Center\", \"Shooting Guard\", \"Power Forward\"])\n    ]\n    dataset.append(row)\n\nrandom.shuffle(dataset)\ntrain_size = int(0.8 * len(dataset))\ntrain_data = dataset[:train_size]\ntest_data = dataset[train_size:]\n\nposition_mapping = {\n    \"Forward\": 0,\n    \"Point Guard\": 1,\n    \"Center\": 2,\n    \"Shooting Guard\": 3,\n    \"Power Forward\": 4\n}\n\ndef prepare_data(data):\n    features = []\n    labels = []\n    for row in data:\n        feature_row = [\n            row[1],\n            row[2],\n            body_type_mapping[row[3]],\n            row[4],\n            row[5],\n            row[6],\n            row[7],\n            row[8],\n            row[9],\n            row[10],\n            row[11],\n            row[12],\n            row[13],\n            row[14],\n            row[15],\n            row[16],\n            row[17],\n            row[18]\n        ]\n        label = position_mapping[row[-1]]\n        features.append(feature_row)\n        labels.append(label)\n    return features, labels\n\ntrain_features, train_labels = prepare_data(train_data)\ntest_features, test_labels = prepare_data(test_data)\n\nclass Perceptron:\n    def __init__(self, num_inputs):\n        self.weights = [0] * num_inputs\n        self.bias = 0\n\n    def predict(self, inputs):\n        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias\n        return 1 if weighted_sum >= 0 else 0\n\n    def train(self, features, labels, learning_rate=0.1, epochs=100):\n        for _ in range(epochs):\n            for i in range(len(features)):\n                prediction = self.predict(features[i])\n                error = labels[i] - prediction\n                for j in range(len(self.weights)):\n                    self.weights[j] += learning_rate * error * features[i][j]\n                self.bias += learning_rate * error\n\nperceptron = Perceptron(len(train_features[0]))\nperceptron.train(train_features, train_labels, learning_rate=0.1, epochs=100)\n\ncorrect_predictions = 0\ntotal_predictions = len(test_features)\n\nfor i in range(total_predictions):\n    prediction = perceptron.predict(test_features[i])\n    if prediction == test_labels[i]:\n        correct_predictions += 1\n\naccuracy = correct_predictions / total_predictions\nprint(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'body_type_mapping' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m         labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features, labels\n\u001b[0;32m---> 73\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m test_features, test_labels \u001b[38;5;241m=\u001b[39m prepare_data(test_data)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPerceptron\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     48\u001b[0m     feature_row \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         row[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     50\u001b[0m         row[\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m---> 51\u001b[0m         \u001b[43mbody_type_mapping\u001b[49m[row[\u001b[38;5;241m3\u001b[39m]],\n\u001b[1;32m     52\u001b[0m         row[\u001b[38;5;241m4\u001b[39m],\n\u001b[1;32m     53\u001b[0m         row[\u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m     54\u001b[0m         row[\u001b[38;5;241m6\u001b[39m],\n\u001b[1;32m     55\u001b[0m         row[\u001b[38;5;241m7\u001b[39m],\n\u001b[1;32m     56\u001b[0m         row[\u001b[38;5;241m8\u001b[39m],\n\u001b[1;32m     57\u001b[0m         row[\u001b[38;5;241m9\u001b[39m],\n\u001b[1;32m     58\u001b[0m         row[\u001b[38;5;241m10\u001b[39m],\n\u001b[1;32m     59\u001b[0m         row[\u001b[38;5;241m11\u001b[39m],\n\u001b[1;32m     60\u001b[0m         row[\u001b[38;5;241m12\u001b[39m],\n\u001b[1;32m     61\u001b[0m         row[\u001b[38;5;241m13\u001b[39m],\n\u001b[1;32m     62\u001b[0m         row[\u001b[38;5;241m14\u001b[39m],\n\u001b[1;32m     63\u001b[0m         row[\u001b[38;5;241m15\u001b[39m],\n\u001b[1;32m     64\u001b[0m         row[\u001b[38;5;241m16\u001b[39m],\n\u001b[1;32m     65\u001b[0m         row[\u001b[38;5;241m17\u001b[39m],\n\u001b[1;32m     66\u001b[0m         row[\u001b[38;5;241m18\u001b[39m]\n\u001b[1;32m     67\u001b[0m     ]\n\u001b[1;32m     68\u001b[0m     label \u001b[38;5;241m=\u001b[39m position_mapping[row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     69\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(feature_row)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'body_type_mapping' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import random\n\nnames = [\"John\", \"Michael\", \"James\", \"David\", \"Robert\", \"Daniel\", \"William\", \"Matthew\", \"Andrew\", \"Christopher\"]\n\ndataset = []\nfor _ in range(200):\n    row = [\n        random.choice(names),\n        random.randint(20, 35),\n        random.uniform(70, 100),\n        random.choice([\"Slim\", \"Medium\", \"Heavy\"]),\n        random.randint(0, 15),\n        random.uniform(20, 50),\n        random.uniform(40, 80),\n        random.uniform(30, 70),\n        random.uniform(40, 80),\n        random.randint(0, 5),\n        random.randint(0, 3),\n        random.uniform(0, 5),\n        random.randint(10, 40),\n        random.uniform(40, 60),\n        random.uniform(20, 40),\n        random.randint(1, 5),\n        random.uniform(15, 25),\n        random.randint(8, 15),\n        random.randint(100, 1000),\n        random.choice([\"Forward\", \"Point Guard\", \"Center\", \"Shooting Guard\", \"Power Forward\"])\n    ]\n    dataset.append(row)\n\nrandom.shuffle(dataset)\ntrain_size = int(0.8 * len(dataset))\ntrain_data = dataset[:train_size]\ntest_data = dataset[train_size:]\n\nposition_mapping = {\n    \"Forward\": 0,\n    \"Point Guard\": 1,\n    \"Center\": 2,\n    \"Shooting Guard\": 3,\n    \"Power Forward\": 4\n}\n\n# Create a mapping for \"Body Type\"\nbody_type_mapping = {\n    \"Slim\": 0,\n    \"Medium\": 1,\n    \"Heavy\": 2\n}\n\ndef prepare_data(data):\n    features = []\n    labels = []\n    for row in data:\n        feature_row = [\n            row[1],\n            row[2],\n            body_type_mapping[row[3]],\n            row[4],\n            row[5],\n            row[6],\n            row[7],\n            row[8],\n            row[9],\n            row[10],\n            row[11],\n            row[12],\n            row[13],\n            row[14],\n            row[15],\n            row[16],\n            row[17],\n            row[18]\n        ]\n        label = position_mapping[row[-1]]\n        features.append(feature_row)\n        labels.append(label)\n    return features, labels\n\ntrain_features, train_labels = prepare_data(train_data)\ntest_features, test_labels = prepare_data(test_data)\n\nclass Perceptron:\n    def __init__(self, num_inputs):\n        self.weights = [0] * num_inputs\n        self.bias = 0\n\n    def predict(self, inputs):\n        weighted_sum = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias\n        return 1 if weighted_sum >= 0 else 0\n\n    def train(self, features, labels, learning_rate=0.1, epochs=100):\n        for _ in range(epochs):\n            for i in range(len(features)):\n                prediction = self.predict(features[i])\n                error = labels[i] - prediction\n                for j in range(len(self.weights)):\n                    self.weights[j] += learning_rate * error * features[i][j]\n                self.bias += learning_rate * error\n\nperceptron = Perceptron(len(train_features[0]))\nperceptron.train(train_features, train_labels, learning_rate=0.1, epochs=100)\n\ncorrect_predictions = 0\ntotal_predictions = len(test_features)\n\nfor i in range(total_predictions):\n    prediction = perceptron.predict(test_features[i])\n    if prediction == test_labels[i]:\n        correct_predictions += 1\n\naccuracy = correct_predictions / total_predictions\nprint(f\"Accuracy on test data: {accuracy * 100:.2f}%\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "text": "Accuracy on test data: 20.00%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Function of Loss and Optimization:\nThe Perceptron uses a simple loss function based on the prediction error. The loss function is minimized using an optimization process that adjusts the weights and bias of the perceptron. In this example, the loss function is simply the prediction error, and optimization is performed in the training loop by updating the weights and bias based on the error. This is a straightforward implementation of a perceptron and does not use more advanced optimization techniques like stochastic gradient descent (SGD) commonly used in more complex neural network models.\n\nExplanation:\n\nIn this code, we have implemented a Perceptron, a fundamental building block of neural networks, to classify basketball player positions. Let's break down the code and explain how it works:\n\nDataset Generation: We start by generating a synthetic dataset with 200 rows and 20 columns. The dataset includes information such as player names, age, weight, body type, years of experience, shooting percentages, defensive stats, and more. It also assigns positions (Forward, Point Guard, etc.) to players.\n\nData Preprocessing: The dataset is shuffled, and 80% of the data is used for training, while the remaining 20% is for testing. We create mappings for positions and body types to convert them into numerical values.\n\nPrepare Data Function: The prepare_data function is used to convert the raw data into a format suitable for training. It converts features like age, weight, and body type into numeric values and maps player positions to numerical labels.\n\nPerceptron Class: The Perceptron class is defined to create and train the perceptron. It has the following methods:\n\n__init__: Initializes the perceptron with weights and bias.\npredict: Predicts the output based on the weighted sum of inputs and a bias. If the sum is greater than or equal to 0, it predicts 1; otherwise, it predicts 0.\ntrain: Trains the perceptron by adjusting the weights and bias using the perceptron learning rule. The number of training epochs and the learning rate can be specified.\nTraining: An instance of the Perceptron class is created with the number of input features, and it is trained on the training data for 100 epochs with a learning rate of 0.1.\n\nTesting: The trained perceptron is then tested on the test data to evaluate its accuracy in predicting player positions.\n\nThe key idea here is that the perceptron learns to classify players into one of the five basketball positions based on the provided features. The weights and bias are adjusted during training to minimize the prediction error, which is a simple form of loss function, and this process is repeated over multiple epochs.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}